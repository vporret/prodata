{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA PROBLEM SET 2\n",
    "# Santi LÃ©geret & Louis Droz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  4]\n",
      " [ 8 32]]\n",
      "Precision: 0.889\n",
      "Recall: 0.800\n",
      "F1: 0.842\n",
      "Accuracy: 0.800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" \\nRecall: In the case of preliminary disease screening of patients for follow-up examinations, the optimal ratio would be at 1.0 as we would like to spot all the patients that have the disease even if it means acceptings a low precision. However the costs of examinations must be low \\nPrecision: In the case of Email Spam Detection, we need a large precision. If the precision is not high, there's a change that important emails are categorized as spam (FP)\\nAccuracy: Accuracy is not a good performance when there is a large class imbalance. Let's stay if it's best to be FP than FP, this particular  key measure doesn't bring any usefulness to the model. \\n\""
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "y_actu = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "y_pred = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "M=confusion_matrix(y_actu, y_pred)\n",
    "print(M)\n",
    "\n",
    "print('Precision: %.3f' % precision_score(y_true=y_actu, y_pred=y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_actu, y_pred=y_pred))\n",
    "print('F1: %.3f' % f1_score(y_true=y_actu, y_pred=y_pred))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_true=y_actu, y_pred=y_pred))\n",
    "\n",
    " \n",
    "# Recall: In the case of preliminary disease screening of patients for follow-up examinations, the optimal ratio \n",
    "        # would be at 1.0 as we would like to spot all the patients that have the disease even if it means acceptings \n",
    "        # a low precision. However the costs of examinations must be low \n",
    "        \n",
    "# Precision: In the case of Email Spam Detection, we need a large precision. If the precision is not high, there's a \n",
    "           # change that important emails are categorized as spam (FP)\n",
    "    \n",
    "# Accuracy: Accuracy is not a good performance when there is a large class imbalance. Let's stay if it's best to be \n",
    "          # FP than FP, this particular  key measure doesn't bring any usefulness to the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Predicted\n",
      "    0   1   2\n",
      "0  20   4   6\n",
      "1   4  16   2\n",
      "2  12   2  32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(\"   Predicted\")\n",
    "conf = pd.DataFrame([[20, 4, 6], [4, 16, 2], [12, 2, 32]]) # Designs the confusion matrix \n",
    "print(conf)\n",
    "\n",
    "# Class 0\n",
    "TP_0 = (conf[0][0]) \n",
    "FP_0 = (conf[1][0] + conf[2][0])\n",
    "FN_0 = (conf[0][1] + conf[0][2])\n",
    "TN_0 = sum(conf.sum(axis = 0)) - TP_0 - FP_0 - FN_0 # True Negs are basically all the rest\n",
    "if (TP_0 + FP_0 + FN_0 + TN_0) != 98: # To ensure no mistakes were made in indexing variables\n",
    "    print(\"Computation mistakes mate\")\n",
    " \n",
    "# Class 1, same process\n",
    "TP_1 = (conf[1][1]) \n",
    "FP_1 = (conf[0][0] + conf[2][1])\n",
    "FN_1 = (conf[2][0] + conf[2][1])\n",
    "TN_1 = sum(conf.sum(axis = 0)) - TP_1 - FP_1 - FN_1\n",
    "if (TP_1 + FP_1 + FN_1 + TN_1) != 98:\n",
    "    print(\"Computation mistakes mate\")\n",
    "    \n",
    "# Class 2, same process\n",
    "TP_2 = (conf[2][2]) \n",
    "FP_2 = (conf[0][2] + conf[1][2])\n",
    "FN_2 = (conf[1][0] + conf[1][2])\n",
    "TN_2 = sum(conf.sum(axis = 0)) - TP_2 - FP_2 - FN_2\n",
    "if (TP_2 + FP_2 + FN_2 + TN_2) != 98:\n",
    "    print(\"Computation mistakes mate\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives of Class 0: 20\n",
      "False Positives of Class 0: 10\n",
      "False Negatives of Class 0: 16\n",
      "True Negatives of Class 0: 52\n"
     ]
    }
   ],
   "source": [
    "print(\"True Positives of Class 0: \" + str(TP_0))\n",
    "print(\"False Positives of Class 0: \" + str(FP_0))\n",
    "print(\"False Negatives of Class 0: \" + str(FN_0))\n",
    "print(\"True Negatives of Class 0: \" + str(TN_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The True Positive Rates for our respective classes are: \n",
      "55.6% , 66.7% & 84.2%\n"
     ]
    }
   ],
   "source": [
    "TPR_0 = (TP_0 / (FN_0 + TP_0))\n",
    "TPR_1 = (TP_1 / (FN_1 + TP_1))\n",
    "TPR_2 = (TP_2 / (FN_2 + TP_2))\n",
    "TPR_0p =\"{0:.1%}\".format(TPR_0)\n",
    "TPR_1p =\"{0:.1%}\".format(TPR_1)\n",
    "TPR_2p =\"{0:.1%}\".format(TPR_2)\n",
    "\n",
    "print(\"The True Positive Rates for our respective classes 0, 1 and 2 are: \\n\" \\\n",
    "      + str(TPR_0p) + \" , \" + str(TPR_1p) + \" & \" + str(TPR_2p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The False Positive Rates for our respective classes 0, 1 and 2 are: \n",
      "16.1% , 29.7% & 23.3%\n"
     ]
    }
   ],
   "source": [
    "FPR_0 = \"{0:.1%}\".format(FP_0 / (TN_0 + FP_0))\n",
    "FPR_1 = \"{0:.1%}\".format(FP_1 / (TN_1 + FP_1))\n",
    "FPR_2 = \"{0:.1%}\".format(FP_2 / (TN_2 + FP_2))\n",
    "\n",
    "print(\"The False Positive Rates for our respective classes 0, 1 and 2 are: \\n\" \\\n",
    "      + str(FPR_0) + \" , \" + str(FPR_1) + \" & \" + str(FPR_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The True Negative Rates for our respective classes 0, 1 and 2 are: \n",
      "83.9% , 70.3% & 76.7%\n"
     ]
    }
   ],
   "source": [
    "TNR_0 = \"{0:.1%}\".format(TN_0 / (FP_0 + TN_0))\n",
    "TNR_1 = \"{0:.1%}\".format(TN_1 / (FP_1 + TN_1))\n",
    "TNR_2 = \"{0:.1%}\".format(TN_2 / (FP_2 + TN_2))\n",
    "\n",
    "print(\"The True Negative Rates for our respective classes 0, 1 and 2 are: \\n\" \\\n",
    "      + str(TNR_0) + \" , \" + str(TNR_1) + \" & \" + str(TNR_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The False Negative Rates for our respective classes 0, 1 and 2 are: \n",
      "44.4% , 33.3% & 15.8%\n"
     ]
    }
   ],
   "source": [
    "FNR_0 = \"{0:.1%}\".format(FN_0 / (TP_0 + FN_0))\n",
    "FNR_1 = \"{0:.1%}\".format(FN_1 / (TP_1 + FN_1))\n",
    "FNR_2 = \"{0:.1%}\".format(FN_2 / (TP_2 + FN_2))\n",
    "\n",
    "print(\"The False Negative Rates for our respective classes 0, 1 and 2 are: \\n\" \\\n",
    "      + str(FNR_0) + \" , \" + str(FNR_1) + \" & \" + str(FNR_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Micro-Average Precision of this set is : 59.6%\n"
     ]
    }
   ],
   "source": [
    "micro_prec = \"{0:.1%}\".format((TP_0 + TP_1 + TP_2)/(FP_0 + FP_1 + FP_2 + TP_0 + TP_1 + TP_2))\n",
    "\n",
    "print(\"The Micro-Average Precision of this set is : \" + str(micro_prec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Macro-Average Recall of this set is : 68.8%\n"
     ]
    }
   ],
   "source": [
    "macro_recall = \"{0:.1%}\".format((TPR_0 + TPR_1 + TPR_2) / 3)\n",
    "\n",
    "print(\"The Macro-Average Recall of this set is : \" + str(macro_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  m/f  Sales  Channel\n",
      "0  0.0    0    0.0        0\n",
      "1  0.2    1    0.8        1\n",
      "2  0.4    1    0.6        2\n",
      "3  0.8    0    0.9        1\n",
      "4  0.0    1    0.2        0\n",
      "5  0.2    0    0.3        0\n",
      "6  1.0    1    0.7        2\n",
      "7  0.0    0    1.0        1\n",
      "8  0.8    1    0.7        2\n",
      "9  0.4    0    0.1        0\n",
      "Train: 10\n",
      "Test: 0\n",
      "[array([0.4, 1. , 0.6, 2. ]), array([0. , 1. , 0.2, 0. ]), array([0.2, 1. , 0.8, 1. ])]\n",
      "[array([0.8, 0. , 0.9, 1. ]), array([1. , 1. , 0.7, 2. ]), array([0.8, 1. , 0.7, 2. ])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lou10x/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/Lou10x/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.DataFrame([[20, 'f', 10, 'E-mail'], # Putting the data into a dataframe\n",
    "                   [30, 'm', 90, 'Phone'],\n",
    "                   [40, 'm', 70, 'Post'],\n",
    "                   [60, 'f', 100, 'Phone'],\n",
    "                   [20, 'm', 30, 'E-mail'],\n",
    "                   [30, 'f', 40, 'E-mail'],\n",
    "                   [70, 'm', 80, 'Post'],\n",
    "                   [20, 'f', 110, 'Phone'],\n",
    "                   [60, 'm', 80, 'Post'],\n",
    "                   [40, 'f', 20, 'E-mail']])\n",
    "\n",
    "df.columns = ['Age', 'm/f', 'Sales', 'Channel'] # Naming the columns\n",
    "\n",
    "\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(df['m/f']))} # Transforming the class m/f into binary variables\n",
    "class_mapping\n",
    "\n",
    "df['m/f'] = df['m/f'].map(class_mapping)\n",
    "\n",
    "\n",
    "\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(df['Channel']))} # Transforming the class Channel into variables\n",
    "class_mapping\n",
    "\n",
    "df['Channel'] = df['Channel'].map(class_mapping)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X1 = df[['Age']].values # Normalizing column Age \n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(X1)\n",
    "df_normalized = pd.DataFrame(np_scaled)\n",
    "\n",
    "\n",
    "X2 = df[['Sales']].values # Normalizing column Sales \n",
    "\n",
    "np_scaled1 = min_max_scaler.fit_transform(X2)\n",
    "df_normalized1 = pd.DataFrame(np_scaled1)\n",
    "\n",
    "df['Age'] = df_normalized\n",
    "df['Sales'] = df_normalized1\n",
    "\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Question b\n",
    "\n",
    "train_data=[]\n",
    "test_data=[]\n",
    "\n",
    "dataset=df.values  # Split the data in case we want to test further the power of prediction of the model. However here we don't need it.\n",
    "for x in range(len(dataset)):\n",
    "        for y in range(3):\n",
    "            dataset[x][y] = float(dataset[x][y])\n",
    "        if random.random() > 0:\n",
    "            train_data.append(dataset[x])\n",
    "        else:\n",
    "            test_data.append(dataset[x])\n",
    "\n",
    "\n",
    "print('Train: ' + repr(len(train_data)))\n",
    "print( 'Test: ' + repr(len(test_data)))\n",
    "\n",
    "\n",
    "def euclideanDistance(instance1, instance2, length): # Set up a function to calculate the Euclidean distance which will permit to define the nearest neigbours\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += pow((instance1[x] - instance2[x]), 2)\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "def getNeighbors(trainingSet, testInstance, k): # Set up the function to identify the neareast neighbours \n",
    "    distances = []\n",
    "    length = len(testInstance)-1\n",
    "    for x in range(len(trainingSet)):\n",
    "        dist = euclideanDistance(testInstance, trainingSet[x], length)\n",
    "        distances.append((trainingSet[x], dist))\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for x in range(k):\n",
    "        neighbors.append(distances[x][0])\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "test1=(0.2,1,0.45,'a')\n",
    "test2=(1.3,0,0.9,'b')\n",
    "\n",
    "k = 3\n",
    "\n",
    "neighbors = getNeighbors(train_data, test1, k)\n",
    "neighbors1 = getNeighbors(train_data, test2, k)\n",
    "\n",
    "print(neighbors)\n",
    "print(neighbors1)\n",
    "\n",
    "# As we can see for the last new customer the majority heads towards 2 which is by post. As for the first new customer it is\n",
    "# undefined as the nearest neigbhours all have the 3 different type of classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up formulas for our bag of words method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "\n",
    "\n",
    "def pull(target):\n",
    "    words = re.sub(\"[^\\w]\", \" \", target).split() # Gets rid of every spaces in the input\n",
    "    return words\n",
    "\n",
    "def tokenize(target_group):\n",
    "    words = []\n",
    "    for target in target_group: # loops through the words to be sorted, activates the cleaning \"pull\" function\n",
    "        keyw = pull(target)\n",
    "        words.extend(keyw) # adds  keywords to the list \n",
    "    \n",
    "    words = sorted(list(set(words))) # removes duplicate (set) and alphabetically sorts the list \n",
    "    return words\n",
    "\n",
    "def baggie(target, words): # requires both the list under review AND the tokenized words\n",
    "    words_processed = pull(target) # calls cleaning function \"pull\"\n",
    "    words_zeros = np.zeros(len(words)) # creates vectors of 0s of lenght of the amount of unique words\n",
    "    for x in words_processed:\n",
    "        for i,word in enumerate(words): # compares the list under review with the unique words \n",
    "            if word == x:\n",
    "                words_zeros[i] += 1 # adds one to the 0 corresponding to the word detected in the vector of 0s\n",
    "            # elif words_zeros    \n",
    "    return np.array(words_zeros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort words and count them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 2 0 0 0 0]\n",
      "[0 1 0 1 0 0 0 0 1]\n",
      "[0 0 1 2 0 0 0 1 0]\n",
      "[1 0 0 0 0 0 0 0 1]\n",
      "[1 0 0 0 1 0 1 1 0]\n",
      "[0 1 0 0 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "target_group = [\"ball goal cart goal theater cart drama drama strategy decision drama \\\n",
    "                 theater ball ball goal player strategy theater cart opera\"]\n",
    "\n",
    "target_unique = tokenize(target_group)\n",
    "\n",
    "# This method is quite heavy, however it allows for a minimum amount do manual input\n",
    "d1_words = [\"ball\", \"cart\", \"goal\", \"goal\"]\n",
    "d2_words = [\"theater\", \"cart\", \"drama\"]\n",
    "d3_words = [\"drama\", \"strategy\", \"decision\", \"drama\"]\n",
    "d4_words = [\"theater\", \"ball\"]\n",
    "d5_words = [\"ball\", \"goal\", \"player\", \"strategy\"]\n",
    "d6_words = [\"theater\", \"cart\", \"opera\"]\n",
    "seperator = \",\"\n",
    "\n",
    "# The first element of our baggie fct allows to transform a list of strings into a simple string readable by the function\n",
    "d1_count = (baggie(seperator.join(d1_words).replace(\",\", \" \"), target_unique)).astype(int)\n",
    "d2_count = (baggie(seperator.join(d2_words).replace(\",\", \" \"), target_unique)).astype(int)\n",
    "d3_count = (baggie(seperator.join(d3_words).replace(\",\", \" \"), target_unique)).astype(int)\n",
    "d4_count = (baggie(seperator.join(d4_words).replace(\",\", \" \"), target_unique)).astype(int)\n",
    "d5_count = (baggie(seperator.join(d5_words).replace(\",\", \" \"), target_unique)).astype(int)\n",
    "d6_count = (baggie(seperator.join(d6_words).replace(\",\", \" \"), target_unique)).astype(int)\n",
    "\n",
    "d1_cat = \"Sports\"\n",
    "d2_cat = \"Culture\"\n",
    "d3_cat = \"Politics\"\n",
    "d4_cat = \"Culture\"\n",
    "d5_cat = \"Sports\"\n",
    "d6_cat = \"Culture\"\n",
    "categories_list = [d1_cat, d2_cat, d3_cat, d4_cat, d5_cat, d6_cat]\n",
    "words_list = [d1_words, d2_words, d3_words, d4_words, d5_words, d6_words]\n",
    "\n",
    "print(d1_count)\n",
    "print(d2_count)\n",
    "print(d3_count)\n",
    "print(d4_count)\n",
    "print(d5_count)\n",
    "print(d6_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the table as requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tWords\t\t\t\t\t\t\t\t\tCategory\n",
      "\tball    cart    decision   drama    goal    opera    player    strategy    theater\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "d1\t 1        1        0         0       2        0        0          0          0\t\t Sports\n",
      "d2\t 0        1        0         1       0        0        0          0          1\t\t Culture\n",
      "d3\t 0        0        1         2       0        0        0          1          0\t\t Politics\n",
      "d1\t 1        0        0         0       0        0        0          0          1\t\t Culture\n",
      "d1\t 1        0        0         0       1        0        1          1          0\t\t Sports\n",
      "d1\t 0        1        0         0       0        1        0          0          1\t\t Culture\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\\t\\tWords\\t\\t\\t\\t\\t\\t\\t\\t\\tCategory\")\n",
    "print(\"\\tball    cart    decision   drama    goal    opera    player    strategy    theater\")\n",
    "print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "print(\"d1\\t \" + str(d1_count[0]) + \"        \" + str(d1_count[1])+ \"        \" + str(d1_count[2])+ \"         \" \\\n",
    "      + str(d1_count[3]) + \"       \" + str(d1_count[4])+ \"        \" + str(d1_count[5])+ \"        \" + str(d1_count[6]) \\\n",
    "      + \"          \" + str(d1_count[7]) + \"          \" + str(d1_count[8]) + \"\\t\\t \" + d1_cat)\n",
    "print(\"d2\\t \" + str(d2_count[0]) + \"        \" + str(d2_count[1])+ \"        \" + str(d2_count[2])+ \"         \" \\\n",
    "      + str(d2_count[3]) + \"       \" + str(d2_count[4])+ \"        \" + str(d2_count[5])+ \"        \" + str(d2_count[6]) \\\n",
    "      + \"          \" + str(d2_count[7]) + \"          \" + str(d2_count[8])+ \"\\t\\t \" + d2_cat)\n",
    "print(\"d3\\t \" + str(d3_count[0]) + \"        \" + str(d3_count[1])+ \"        \" + str(d3_count[2])+ \"         \" \\\n",
    "      + str(d3_count[3]) + \"       \" + str(d3_count[4])+ \"        \" + str(d3_count[5])+ \"        \" + str(d3_count[6]) \\\n",
    "      + \"          \" + str(d3_count[7]) + \"          \" + str(d3_count[8])+ \"\\t\\t \" + d3_cat)\n",
    "print(\"d1\\t \" + str(d4_count[0]) + \"        \" + str(d4_count[1])+ \"        \" + str(d4_count[2])+ \"         \" \\\n",
    "      + str(d4_count[3]) + \"       \" + str(d4_count[4])+ \"        \" + str(d4_count[5])+ \"        \" + str(d4_count[6]) \\\n",
    "      + \"          \" + str(d4_count[7]) + \"          \" + str(d4_count[8])+ \"\\t\\t \" + d4_cat)\n",
    "print(\"d1\\t \" + str(d5_count[0]) + \"        \" + str(d5_count[1])+ \"        \" + str(d5_count[2])+ \"         \" \\\n",
    "      + str(d5_count[3]) + \"       \" + str(d5_count[4])+ \"        \" + str(d5_count[5])+ \"        \" + str(d5_count[6]) \\\n",
    "      + \"          \" + str(d5_count[7]) + \"          \" + str(d5_count[8])+ \"\\t\\t \" + d5_cat)\n",
    "print(\"d1\\t \" + str(d6_count[0]) + \"        \" + str(d6_count[1])+ \"        \" + str(d6_count[2])+ \"         \" \\\n",
    "      + str(d6_count[3]) + \"       \" + str(d6_count[4])+ \"        \" + str(d6_count[5])+ \"        \" + str(d6_count[6]) \\\n",
    "      + \"          \" + str(d6_count[7]) + \"          \" + str(d6_count[8])+ \"\\t\\t \" + d6_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this section is to count how many documents are part of one class\n",
    "sports_doc = 0\n",
    "culture_doc = 0\n",
    "politics_doc = 0\n",
    "documents = len(categories_list)\n",
    "\n",
    "for i,word in enumerate(categories_list): # loop to count how many documents of each class\n",
    "            if word == \"Sports\":\n",
    "                sports_doc += 1\n",
    "            elif word == \"Culture\":\n",
    "                culture_doc += 1\n",
    "            elif word == \"Politics\":\n",
    "                politics_doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Probabilities computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of class Sports is 33.3%\n",
      "The probability of class Culture is 50.0%\n",
      "The probability of class Politics is 16.7%\n"
     ]
    }
   ],
   "source": [
    "# Compute individual probabilities, essentially the (# Documents from each class) / (# Documents)\n",
    "prob_Sports = \"{0:.1%}\".format(sports_doc / documents) \n",
    "prob_Culture = \"{0:.1%}\".format(culture_doc / documents)\n",
    "prob_Politics = \"{0:.1%}\".format(politics_doc / documents)\n",
    "\n",
    "print(\"The probability for class Sports is \" + str(prob_Sports))\n",
    "print(\"The probability for class Culture is \" + str(prob_Culture))\n",
    "print(\"The probability for class Politics is \" + str(prob_Politics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section aims at creating lists of words per class as well as computing the denominator of the conditional prob\n",
    "occur_Sports = 0 # amount of word occurences per class\n",
    "occur_Culture = 0\n",
    "occur_Politics = 0 \n",
    "sports = [] # lists of all words mentioned within a class\n",
    "culture = []\n",
    "politics = []\n",
    "\n",
    "for i,word in enumerate(categories_list): # loop to count the amount of word occurences per class\n",
    "            if word == \"Sports\":\n",
    "                occur_Sports += len(words_list[i])\n",
    "                sports.append(words_list[i]) # regroups all words linked to the class within same list\n",
    "            elif word == \"Culture\":\n",
    "                occur_Culture += len(words_list[i])\n",
    "                culture.append(words_list[i])\n",
    "            elif word == \"Politics\":\n",
    "                occur_Politics += len(words_list[i])\n",
    "                politics.append(words_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ball', 'ball', 'cart', 'goal', 'goal', 'goal', 'player', 'strategy']\n",
      "['theater', 'cart', 'drama', 'theater', 'ball', 'theater', 'cart', 'opera']\n",
      "['drama', 'strategy', 'decision', 'drama']\n"
     ]
    }
   ],
   "source": [
    "# Loop to flatten the nested lists of words per class, due to previous step \n",
    "\n",
    "sports_flat=[]\n",
    "\n",
    "for x in sports:\n",
    "    for y in x:\n",
    "        sports_flat.append(y)\n",
    "        \n",
    "culture_flat=[]\n",
    "\n",
    "for x in culture:\n",
    "    for y in x:\n",
    "        culture_flat.append(y)\n",
    "        \n",
    "politics_flat=[]\n",
    "\n",
    "for x in politics:\n",
    "    for y in x:\n",
    "        politics_flat.append(y)\n",
    "\n",
    "print(sports_sorted)\n",
    "print(culture_flat)\n",
    "print(politics_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count occurences of each word per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "occur_ball_sports = sports_flat.count(\"ball\")\n",
    "occur_cart_sports = sports_flat.count(\"cart\")\n",
    "occur_goal_sports = sports_flat.count(\"goal\")\n",
    "occur_player_sports = sports_flat.count(\"player\")\n",
    "occur_strategy_sports = sports_flat.count(\"strategy\")\n",
    "\n",
    "occur_theater_culture = culture_flat.count(\"theater\")\n",
    "occur_cart_culture = culture_flat.count(\"cart\")\n",
    "occur_drama_culture = culture_flat.count(\"drama\")\n",
    "occur_ball_culture = culture_flat.count(\"ball\")\n",
    "occur_opera_culture = culture_flat.count(\"opera\")\n",
    "\n",
    "occur_drama_politics = politics_flat.count(\"drama\")\n",
    "occur_strategy_politics = politics_flat.count(\"strategy\")\n",
    "occur_decision_politics = politics_flat.count(\"decision\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conditional probability of reading the word *ball* in class *sports* is: 25.0%\n",
      "The conditional probability of reading the word *cart* in class *sports* is: 12.5%\n",
      "The conditional probability of reading the word *goal* in class *sports* is: 37.5%\n",
      "The conditional probability of reading the word *player* in class *sports* is: 12.5%\n",
      "The conditional probability of reading the word *strategy* in class *sports* is: 12.5%\n",
      "--------------------------------------------------------------------------------------------\n",
      "The conditional probability of reading the word *theater* in class *culture* is: 37.5%\n",
      "The conditional probability of reading the word *cart* in class *culture* is: 25.0%\n",
      "The conditional probability of reading the word *drama* in class *culture* is: 12.5%\n",
      "The conditional probability of reading the word *ball* in class *culture* is: 12.5%\n",
      "The conditional probability of reading the word *opera* in class *culture* is: 12.5%\n",
      "--------------------------------------------------------------------------------------------\n",
      "The conditional probability of reading the word *drama* in class *politics* is: 25.0%\n",
      "The conditional probability of reading the word *strategy* in class *politics* is: 12.5%\n",
      "The conditional probability of reading the word *decision* in class *politics* is: 12.5%\n"
     ]
    }
   ],
   "source": [
    "# Using the formula (# occurences of w in class c) / (# word occurences in class c)\n",
    "prob_ball_sports = \"{0:.1%}\".format(occur_ball_sports / occur_Sports)\n",
    "prob_cart_sports = \"{0:.1%}\".format(occur_cart_sports / occur_Sports)\n",
    "prob_goal_sports = \"{0:.1%}\".format(occur_goal_sports / occur_Sports)\n",
    "prob_player_sports = \"{0:.1%}\".format(occur_player_sports / occur_Sports)\n",
    "prob_strategy_sports = \"{0:.1%}\".format(occur_strategy_sports / occur_Sports)\n",
    "\n",
    "prob_theater_culture = \"{0:.1%}\".format(occur_theater_culture / occur_Culture)\n",
    "prob_cart_culture = \"{0:.1%}\".format(occur_cart_culture / occur_Culture)\n",
    "prob_drama_culture = \"{0:.1%}\".format(occur_drama_culture / occur_Culture)\n",
    "prob_ball_culture = \"{0:.1%}\".format(occur_ball_culture / occur_Culture)\n",
    "prob_opera_culture = \"{0:.1%}\".format(occur_opera_culture / occur_Culture)\n",
    "\n",
    "prob_drama_politics = \"{0:.1%}\".format(occur_drama_politics / occur_Culture)\n",
    "prob_strategy_politics = \"{0:.1%}\".format(occur_strategy_politics / occur_Culture)\n",
    "prob_decision_politics = \"{0:.1%}\".format(occur_decision_politics / occur_Culture)\n",
    "\n",
    "print(\"The conditional probability of reading the word *ball* in class *sports* is: \" + str(prob_ball_sports))\n",
    "print(\"The conditional probability of reading the word *cart* in class *sports* is: \" + str(prob_cart_sports))\n",
    "print(\"The conditional probability of reading the word *goal* in class *sports* is: \" + str(prob_goal_sports))\n",
    "print(\"The conditional probability of reading the word *player* in class *sports* is: \" + str(prob_player_sports))\n",
    "print(\"The conditional probability of reading the word *strategy* in class *sports* is: \" + str(prob_strategy_sports))\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "print(\"The conditional probability of reading the word *theater* in class *culture* is: \" + str(prob_theater_culture))\n",
    "print(\"The conditional probability of reading the word *cart* in class *culture* is: \" + str(prob_cart_culture))\n",
    "print(\"The conditional probability of reading the word *drama* in class *culture* is: \" + str(prob_drama_culture))\n",
    "print(\"The conditional probability of reading the word *ball* in class *culture* is: \" + str(prob_ball_culture))\n",
    "print(\"The conditional probability of reading the word *opera* in class *culture* is: \" + str(prob_opera_culture))\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "print(\"The conditional probability of reading the word *drama* in class *politics* is: \" + str(prob_drama_politics))\n",
    "print(\"The conditional probability of reading the word *strategy* in class *politics* is: \" + str(prob_strategy_politics))\n",
    "print(\"The conditional probability of reading the word *decision* in class *politics* is: \" + str(prob_decision_politics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Roles': {5: 'Action',\n",
      "           15: {'Duration (min)': {100: 'Action', 120: 'Drama'}},\n",
      "           20: 'Drama'}}\n",
      "Drama\n",
      "Action\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "eps = np.finfo(float).eps\n",
    "from numpy import log2 as log\n",
    "\n",
    "df = pd.DataFrame([[5, 80, 'no', 'Action'], # Putting the data into a dataframe\n",
    "                       [15, 120, 'yes', 'Drama'],\n",
    "                       [15, 100,'yes', 'Action'],\n",
    "                       [20, 80, 'no', 'Drama'],\n",
    "                       [5, 80, 'no', 'Action']])\n",
    "\n",
    "df.columns = ['Roles', 'Duration (min)', 'Audiobook', 'Genre']\n",
    "\n",
    "\n",
    "\n",
    "def find_entropy(df): # Finding the attribute of the desired class (genre)\n",
    "    Class = df.keys()[-1]   \n",
    "    entropy = 0\n",
    "    values = df[Class].unique()\n",
    "    for value in values:\n",
    "        fraction = df[Class].value_counts()[value]/len(df[Class])\n",
    "        entropy += -fraction*np.log2(fraction)\n",
    "    return entropy\n",
    "  \n",
    "  \n",
    "def find_entropy_attribute(df,attribute): # Finding the entropy of the attribute in order to calculate Information Gain\n",
    "    Class = df.keys()[-1]  \n",
    "    target_variables = df[Class].unique()  \n",
    "    variables = df[attribute].unique()   \n",
    "    entropy2 = 0\n",
    "    for variable in variables:\n",
    "        entropy = 0\n",
    "        for target_variable in target_variables:\n",
    "            num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n",
    "            den = len(df[attribute][df[attribute]==variable])\n",
    "            fraction = num/(den+eps)\n",
    "            entropy += -fraction*log(fraction+eps)\n",
    "        fraction2 = den/len(df)\n",
    "        entropy2 += -fraction2*entropy\n",
    "    return abs(entropy2)\n",
    "\n",
    "def find_winner(df): #Setting-up the code to find the winners and calculate Information Gain\n",
    "    Entropy_att = []\n",
    "    IG = []\n",
    "    for key in df.keys()[:-1]:\n",
    "        IG.append(find_entropy(df)-find_entropy_attribute(df,key))\n",
    "    return df.keys()[:-1][np.argmax(IG)]\n",
    "  \n",
    "def get_subtable(df, node,value): #Getting the subtable to separate classes\n",
    "    return df[df[node] == value].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def buildTree(df,tree=None): \n",
    "    Class = df.keys()[-1]  \n",
    "    node = find_winner(df)\n",
    "    \n",
    "    attValue = np.unique(df[node])\n",
    "\n",
    "    #Create an empty dictionary to create tree    \n",
    "    if tree is None:                    \n",
    "        tree={}\n",
    "        tree[node] = {}\n",
    "    \n",
    "   #We make a loop to construct the tree with regards to the purity \n",
    "\n",
    "    for value in attValue:\n",
    "        \n",
    "        subtable = get_subtable(df,node,value)\n",
    "        clValue,counts = np.unique(subtable['Genre'],return_counts=True)                        \n",
    "        \n",
    "        if len(counts)==1:#Checking purity \n",
    "            tree[node][value] = clValue[0]                                                    \n",
    "        else:        \n",
    "            tree[node][value] = buildTree(subtable)\n",
    "                   \n",
    "    return tree\n",
    "\n",
    "tree = buildTree(df)\n",
    "\n",
    "import pprint # We see here that audiobook is not implemented in the tree as it doesn't give any information about the genre. \n",
    "pprint.pprint(tree)\n",
    "\n",
    "\n",
    "def predict(inst,tree): #Function built to go through the tree recusirvely. \n",
    "\n",
    "    for nodes in tree.keys():        \n",
    "        \n",
    "        value = inst[nodes]\n",
    "        tree = tree[nodes][value]\n",
    "        prediction = 0\n",
    "            \n",
    "        if type(tree) is dict:\n",
    "            prediction = predict(inst, tree)\n",
    "        else:\n",
    "            prediction = tree\n",
    "            break;                            \n",
    "        \n",
    "    return prediction\n",
    "\n",
    "\n",
    "# In order to predict, we had to change the data given for the movies as the parameters didn't correpond.\n",
    "# We took the closest values to estimate the genre\n",
    "\n",
    "inst1=pd.Series({'Roles':15,'Duration (min)': 120, 'Audiobook':'yes'}) \n",
    "\n",
    "prediction1=predict(inst1, tree)\n",
    "print(prediction1)\n",
    "\n",
    "inst2=pd.Series({'Roles':5,'Duration (min)': 120, 'Audiobook':'yes'})\n",
    "\n",
    "prediction1=predict(inst2, tree)\n",
    "print(prediction1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write our models, $f_n(x)$, in a classical fashion, i.e: the exact value tweaked by an error term. Remaining classical, we also compute the squared errors. \n",
    "\n",
    "$f_n(x) = y(x) + \\epsilon_n(x)$\n",
    "\n",
    "$\\mathbb{E}_{x} [\\{ f_n(x) - y(x)\\}^2] = \\mathbb{E}_{x} [\\epsilon_n(x)^2]$\n",
    "\n",
    "The mean squared errors can then be written:\n",
    "\n",
    "$E_{avg} = \\frac{1}{N} \\sum_{n=1}^{N}\\mathbb{E}_{x} [\\epsilon_n(x)^2]] $\n",
    "\n",
    "We can the proceed to the models aggregation:\n",
    "\n",
    "$f_{agg}(X)= \\frac{1}{N} \\sum_{n=1}^{N} f_n(x)$\n",
    "$\\mathbb{E}_{agg} = \\mathbb{E}_{X} \\bigg[ \\bigg\\{ \\frac{1}{N} \\sum_{n=1}^{N} \\epsilon_n(x) \\bigg\\}^2 \\bigg] $\n",
    "\n",
    "The aggregated error thus ensues:\n",
    "\n",
    "$E_{agg}= \\mathbb{E}_{X} \\bigg[ \\bigg\\{ \\frac{1}{N} \\sum_{n=1}^{N} \\epsilon_n(x) \\bigg\\}^2 \\bigg] = \\frac{1}{N^2} \\sum_{n=1}^{N} \\sum_{i=1}^{I} \\mathbb{E}_{X} [ \\epsilon_n(x) \\epsilon_i(x)]$\n",
    "\n",
    "We do assume errors to be uncorrelated:\n",
    "\n",
    "$  \\mathbb{E}_{X} [ \\epsilon_n(x) \\epsilon_i(x)] = 0$ when n $\\neq$ i, then: \n",
    "\n",
    "$E_{agg} = \\frac{1}{i^2} \\sum_{i=1}^{I} \\mathbb{E}_{X} [ \\epsilon_i(x)^2] = \\frac{1}{I} E_{avg}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Replacing with the given data:\n",
    "\n",
    "$E_{avg1} = 0.3$      $E_{avg2} = 0.2$      $E_{avg3} = 0.1$     MI = 3\n",
    "\n",
    "The estimate of the ensemble learning error is therefore:\n",
    "\n",
    "$E_{agg} = 1/3^2 * (0.3 + 0.2 + 0.1) = 0.06667$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-374-07e5db4f2d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\s+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('data.txt', header=None,sep='\\s+') # Import data\n",
    "\n",
    "BT = data[0] # Separate data into the different variables\n",
    "WBC = data[1]\n",
    "DS = data[2]\n",
    "I = data[3]\n",
    "\n",
    "\n",
    "data_infected = [] # Separate the data depending on infection\n",
    "data_healthy = []\n",
    "for i in range (len(data)):\n",
    "    if data.iloc[i, 3] > 0:\n",
    "        data_infected.append(data.iloc[i,:])\n",
    "    else:\n",
    "        data_healthy.append(data.iloc[i,:])\n",
    "\n",
    "\n",
    "BT_healthy = [] # Create subcategory in each variable depending on infection or not\n",
    "for i in range (len(data_healthy)):\n",
    "    BT_healthy.append(data_healthy [i][0])\n",
    "    \n",
    "WBC_healthy = []    \n",
    "for i in range (len(data_healthy)):\n",
    "    WBC_healthy.append(data_healthy [i][1])\n",
    "    \n",
    "DS_healthy = []    \n",
    "for i in range (len(data_healthy)):\n",
    "    DS_healthy.append(data_healthy [i][2])\n",
    "    \n",
    "BT_infected = []    \n",
    "for i in range (len(data_infected)):\n",
    "    BT_infected.append(data_infected [i][0])\n",
    "    \n",
    "WBC_infected = []    \n",
    "for i in range (len(data_infected)):\n",
    "    WBC_infected.append(data_infected [i][1])\n",
    "    \n",
    "DS_infected = []    \n",
    "for i in range (len(data_infected)):\n",
    "    DS_infected.append(data_infected [i][2])\n",
    "    \n",
    "    \n",
    "fig = plt.figure() # Plotting the graphs accordingly\n",
    "Graph1 = fig.add_subplot(111)\n",
    "Graph1.scatter(BT_healthy, WBC_healthy, s=2, c='b', marker=\"x\", label='Healthy')\n",
    "Graph1.scatter(BT_infected,WBC_infected, s=2, c='r', marker=\"o\", label='Infected')\n",
    "plt.legend(loc='upper left');\n",
    "plt.title('BT - WBC')\n",
    "plt.xlabel('Body Temperature [cÂ°]')\n",
    "plt.ylabel('White Blood Cells Count')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "Graph1 = fig.add_subplot(111)\n",
    "Graph1.scatter(DS_healthy, WBC_healthy, s=2, c='b', marker=\"x\", label='Healthy')\n",
    "Graph1.scatter(DS_infected,WBC_infected, s=2, c='r', marker=\"o\", label='Infected')\n",
    "plt.legend(loc='upper left');\n",
    "plt.title('WBC - DS')\n",
    "plt.xlabel('Daily Sleep [Hours]')\n",
    "plt.ylabel('White Blood Cells Count')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "Graph1 = fig.add_subplot(111)\n",
    "Graph1.scatter(DS_healthy, BT_healthy, s=2, c='b', marker=\"x\", label='Healthy')\n",
    "Graph1.scatter(DS_infected,BT_infected, s=2, c='r', marker=\"o\", label='Infected')\n",
    "plt.legend(loc='upper left');\n",
    "plt.title('DS - BT')\n",
    "plt.xlabel('Daily Sleep [Hours]')\n",
    "plt.ylabel('Body Temperature [cÂ°]')\n",
    "plt.show()\n",
    "\n",
    "# Part B \n",
    "\n",
    "\n",
    "NormWBC_infected = preprocessing.normalize([WBC_infected]) # Normalizing data\n",
    "NormWBC_healthy = preprocessing.normalize([WBC_healthy])\n",
    "x = NormWBC_healthy.tolist()\n",
    "y = NormWBC_infected.tolist()\n",
    "\n",
    "fig = plt.figure() # Plotting the graphs accordingly\n",
    "plt.hist(WBC_healthy, bins = 50, facecolor = 'blue')\n",
    "plt.hist(WBC_infected, bins = 50, facecolor = 'red')\n",
    "plt.title('White Blood Count CH & CI')\n",
    "plt.xlabel('White Blood Count')\n",
    "plt.ylabel('Recurrence')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(BT_healthy, bins = 50, facecolor = 'blue')\n",
    "plt.hist(BT_infected, bins = 50, facecolor = 'red')\n",
    "plt.title('Body Tempereature CH & CI')\n",
    "plt.xlabel('Body Tempereature [cÂ°]')\n",
    "plt.ylabel('Recurrence')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(DS_infected, bins = 50, facecolor = 'red')\n",
    "plt.hist(DS_healthy, bins = 50, facecolor = 'blue')\n",
    "plt.title('Daily Sleep CH & CI')\n",
    "plt.xlabel('Daily Sleep [Hours]')\n",
    "plt.ylabel('Recurrence')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Answer to Part C: \n",
    "\n",
    "# There are two attributes that stand out in the graphs regarding the infection status: White Blood Count and Body Temperature.\n",
    "# This is based on two critera, the mean and the standard deviation that are quite different from the CH and CI. This can be noticed visually.\n",
    "# However if I had too choose only one, I would choose the White Blood Count as the two distributions seem to be the least overlapping ones.\n",
    "\n",
    "\n",
    "# Part D:\n",
    "\n",
    "Corr = data.corr()\n",
    "print(Corr)\n",
    "\n",
    "#White Blood Count is the one with the highest correlation to the infection status and it is consistent with our previsous reasonning. \n",
    "\n",
    "# Part E: \n",
    "\n",
    "\n",
    "fig = plt.figure() # Plotting the graphs accordingly\n",
    "Graph1 = fig.add_subplot(111)\n",
    "Graph1.scatter(DS,I, s=2, c='b', marker=\"x\")\n",
    "plt.title('DailySleep vs Infection status')\n",
    "plt.xlabel('Daily Sleep [Hours]')\n",
    "plt.ylabel('Infection')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "Graph1 = fig.add_subplot(111)\n",
    "Graph1.scatter(WBC,I, s=2, c='b', marker=\"x\")\n",
    "plt.title('White Blood Cells counts vs Infection status')\n",
    "plt.xlabel('White Blood Cells Count')\n",
    "plt.ylabel('Infection')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "Graph1 = fig.add_subplot(111)\n",
    "Graph1.scatter(BT,I, s=2, c='b', marker=\"x\")\n",
    "plt.title('Body Temperature vs Infection status')\n",
    "plt.xlabel('Body Temperature [cÂ°]')\n",
    "plt.ylabel('Infection')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# We see the correlation reflected in the none overlapping feature of the infectious and non-infectious persons. \n",
    "# Indeed, it is almoste as if the chart was separeted in two part. A 100% correlation would in that case give the aformentionned result.\n",
    "# This graph is a good basis to construct a logit or probit regression model. Indeed, a clear separation would probably give\n",
    "# decent explanatory and predictions results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-375-6e2e62ba9c3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/seaborn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Capture the original matplotlib rcParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0m_orig_rc_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Import seaborn objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Inspection of the dataset\n",
    "\n",
    "%reset -f\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = fetch_20newsgroups()\n",
    "data.target_names\n",
    "\n",
    "religions = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "print(train.data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-376-7a3e9b174431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "mod = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "mod.fit(train.data, train.target)\n",
    "stickies = mod.predict(test.data)\n",
    "\n",
    "\n",
    "conf = confusion_matrix(test.target, stickies)\n",
    "sns.heatmap(conf.T, annot=True, fmt='d',\n",
    "            xticklabels = train.target_names, yticklabels = train.target_names)\n",
    "\n",
    "plt.xlabel(\"True Stickies\")\n",
    "plt.ylabel(\"Predicted Stickies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-377-a1f42073a126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_category\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "def predict_category(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]\n",
    "\n",
    "\n",
    "\n",
    "predict_category('determining the screen resolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-378-b7c4cd2634e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminant_analysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "x1 = [1, 5, 1.5, 8, 1, 9]\n",
    "x2 = [ 2, 8, 1.8, 8, 0.6, 11]\n",
    "X = np.array([[1, 5, 1.5, 8, 1, 9],[2, 8, 1.8, 8, 0.6, 11]])\n",
    "X = X.transpose()\n",
    "y = np.array([0,1,0,1,0,1])\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "Graph1 = fig.add_subplot(111)\n",
    "colormap = np.array(['r', 'k'])\n",
    "Graph1.scatter(x1,x2, s=50, c=colormap[y], marker=\"o\")\n",
    "plt.title('x1 vs x2')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()\n",
    "\n",
    "# Looking at the chart we see that they are two definite type of clusters: one on the lower left corner\n",
    "# and the other one on the upper left corner. \n",
    "\n",
    "# LDA\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_1 = clf.fit(X, y).transform(X)\n",
    "\n",
    "\n",
    "plt.scatter(x1,x2, s=50, c=colormap[y], marker=\"o\")\n",
    "ymin, ymax = plt.ylim()\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(ymin, ymax)\n",
    "yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,10])\n",
    "axes.set_ylim([0,12])\n",
    "plt.plot(yy,xx, 'k-', color='green', label='LDA')\n",
    "\n",
    "\n",
    "# Perceptron\n",
    "\n",
    "Per = Perceptron(tol=1e-3, random_state=0)\n",
    "X_2 = Per.fit(X, y)\n",
    "\n",
    "Per.score(X, y) \n",
    "\n",
    "plt.scatter(x1,x2, s=50, c=colormap[y], marker=\"o\")\n",
    "ymin, ymax = plt.ylim()\n",
    "w = Per.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(ymin, ymax)\n",
    "yy = a * xx - (Per.intercept_[0]) / w[1]\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,10])\n",
    "axes.set_ylim([0,12])\n",
    "plt.plot(yy,xx, 'k-', color='black', label='Per')\n",
    "plt.legend()\n",
    "\n",
    "# SVM\n",
    "\n",
    "SV = SVC(gamma='auto', kernel='linear')\n",
    "SV.fit(X, y)\n",
    "\n",
    "plt.scatter(x1,x2, s=50, c=colormap[y], marker=\"o\")\n",
    "ymin, ymax = plt.ylim()\n",
    "w = SV.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(ymin, ymax)\n",
    "yy = a * xx - (SV.intercept_[0]) / w[1]\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,10])\n",
    "axes.set_ylim([0,12])\n",
    "plt.plot(yy,xx, 'k-', color='yellow', label='SVM')\n",
    "plt.legend()\n",
    "\n",
    "# Predictions\n",
    "\n",
    "print('LDA prediction: ',clf.predict([[0.58, 0.76]]))\n",
    "print('Perceptron prediction: ',Per.predict([[0.58, 0.76]]))\n",
    "print('SVM prediction: ',SV.predict([[0.58, 0.76]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
